{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import WebKB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WebKB(root='./', name='Cornell')\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transpose original label to binary label, and do train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[165, 1703], edge_index=[2, 257], y=[165], train_mask=[165, 10], val_mask=[165, 10], test_mask=[165, 10])\n"
     ]
    }
   ],
   "source": [
    "data.y[data.y > 0] = 1\n",
    "data_gt = data.y.detach().cpu().numpy()\n",
    "\n",
    "n_node = len(data.x)\n",
    "n_test = n_node // 10\n",
    "\n",
    "all_idx = list(range(n_node))\n",
    "random.shuffle(all_idx)\n",
    "test_index = all_idx[:n_test]\n",
    "train_index = all_idx[n_test:]\n",
    "train_data = data.subgraph(torch.LongTensor(train_index))\n",
    "\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the network to do classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class RelationClassifier(nn.Module):\n",
    "    def __init__(self, nodecls, zinput_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.nodecls = nodecls\n",
    "        self.fcz = nn.Linear(zinput_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x, z):\n",
    "        x = self.nodecls(x)\n",
    "        z = self.fcz(z)\n",
    "        return (x + z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1. Train $\\phi_1(f_v)$ to predict $Y_v$ based on $f_v$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = data.x.shape[1]\n",
    "output_dim = 2\n",
    "\n",
    "model_f = NodeClassifier(input_dim, output_dim)\n",
    "optim_f = optim.Adam(model_f.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 833.39it/s]\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model_f.train()\n",
    "    optim_f.zero_grad()\n",
    "\n",
    "    logits = model_f(train_data.x)\n",
    "    loss = loss_fn(logits, train_data.y)\n",
    "\n",
    "    loss.backward()\n",
    "    optim_f.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2. Train $\\phi(f_v,z_v)$ to predict $Y_v$ based on $f_v$ and summary $z_v$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_z = RelationClassifier(model_f, 4, output_dim)\n",
    "optim_z = optim.Adam(model_z.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build neighbouring feature $z_v$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.feature_z = torch.zeros(train_data.num_nodes, 4)\n",
    "for v in range(train_data.num_nodes):\n",
    "    in_neighs = train_data.edge_index[1, train_data.edge_index[1] == v]\n",
    "    out_neighs = train_data.edge_index[1, train_data.edge_index[0] == v]\n",
    "    in_labels = set(train_data.y[in_neighs].detach().numpy())\n",
    "    out_labels = set(train_data.y[out_neighs].detach().numpy())\n",
    "    \n",
    "    train_data.feature_z[v] = torch.tensor([\n",
    "        0 in in_labels,\n",
    "        1 in in_labels,\n",
    "        0 in out_labels,\n",
    "        1 in out_labels\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 1000.12it/s]\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model_z.train()\n",
    "    optim_z.zero_grad()\n",
    "\n",
    "    logits = model_z(train_data.x, train_data.feature_z)\n",
    "    loss = loss_fn(logits, train_data.y)\n",
    "\n",
    "    loss.backward()\n",
    "    optim_z.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1. Set initial $Y_v$ by $\\phi_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8032786885245902\n"
     ]
    }
   ],
   "source": [
    "model_f.eval()\n",
    "with torch.no_grad():\n",
    "    data.y = model_f(data.x).argmax(dim=1)\n",
    "accuracy = accuracy_score(data_gt, data.y)\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2. Repeat the iteration:\n",
    "- Update $z_v$ by $Y_{N(v)}$\n",
    "- Update $Y_v$ with new $z_v$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 47.10it/s]\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for epoch in tqdm(range(epochs)): \n",
    "    data.feature_z = torch.zeros(data.num_nodes, 4)\n",
    "    for v in range(data.num_nodes):\n",
    "        in_neighs = data.edge_index[1, data.edge_index[1] == v]\n",
    "        out_neighs = data.edge_index[1, data.edge_index[0] == v]\n",
    "        in_labels = set(data.y[in_neighs].detach().numpy())\n",
    "        out_labels = set(data.y[out_neighs].detach().numpy())\n",
    "        \n",
    "        data.feature_z[v] = torch.tensor([\n",
    "            0 in in_labels,\n",
    "            1 in in_labels,\n",
    "            0 in out_labels,\n",
    "            1 in out_labels\n",
    "        ])\n",
    "    \n",
    "    model_z.eval()\n",
    "    with torch.no_grad():\n",
    "        data.y = model_z(data.x, data.feature_z).argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8633879781420765\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(data_gt, data.y)\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: In this case, iterative classification performs better than classification with only features, however\n",
    "- The models are not well trained, because in this dataset, when train epoch is set larger, even classification with only features can reach acc 100%\n",
    "- Sometimes iterative classification performs worse, because dimension of additional features(4) is greatly less than that of initial feature(1703)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gomoku",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
